{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sklearn.datasets\n",
    "import nltk.stem\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "import scipy as sp\n",
    "import PyPDF2\n",
    "import docx\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import string\n",
    "import docx\n",
    "import docx2txt\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('english')\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_operation():\n",
    "    data = []\n",
    "    mypath = \"Users/sahebsingh/Desktop/Projects/Honeywell/Honeywell\" #Path to folder/Directory\n",
    "    \n",
    "    list_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    for k in range(len(list_files)):\n",
    "        document = docx.Document(list_files[k])\n",
    "    print (document.paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPdf(file):\n",
    "    data = []\n",
    "    file = open(file, 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(file)\n",
    "    page_number = pdfReader.numPages\n",
    "    for i in range(0, page_number):\n",
    "        pageObj = pdfReader.getPage(i)\n",
    "        data.append(pageObj.extractText())\n",
    "        \n",
    "    return '\\n'.join(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocx(filename):\n",
    "    my_text = docx2txt.process(filename)\n",
    "    \n",
    "    return np.array(my_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = getDocx('2.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText1(filename):\n",
    "    from docx import Document\n",
    "    document = Document(filename)\n",
    "    for para in document.paragraphs:\n",
    "        print(para.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "data = getPdf('2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "articleURL = \"http://curia.europa.eu/juris/document/document.jsf?text=&docid=139407&pageIndex=0&doclang=EN&mode=lst&dir=&occ=first&part=1&cid=52454\"\n",
    "def getText(url):\n",
    "    page = urlopen(url).read().decode('utf8', 'ignore')\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
    "    return text.encode('ascii', errors='replace').decode().replace(\"?\",\"\")\n",
    "text = getText(articleURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "def summarize(text, n):\n",
    "    sents = sent_tokenize(text)\n",
    "    \n",
    "    assert n <= len(sents)\n",
    "    wordSent = word_tokenize(text.lower())\n",
    "    stopWords = set(stopwords.words('english')+list(punctuation))\n",
    "    \n",
    "    wordSent= [word for word in wordSent if word not in stopWords]\n",
    "    freq = FreqDist(wordSent)\n",
    "    ranking = defaultdict(int)\n",
    "    \n",
    "    for i, sent in enumerate(sents):\n",
    "        for w in word_tokenize(sent.lower()):\n",
    "            if w in freq:\n",
    "                ranking[i] += freq[w]\n",
    "    sentsIDX = nlargest(n, ranking, key=ranking.get)\n",
    "    return [sents[j] for j in sorted(sentsIDX)]\n",
    "summaryArr = summarize(data, 10)\n",
    "# summaryArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 8.299\n",
      "Iteration  1, inertia 4.837\n",
      "Converged at iteration 1: center shift 0.000000e+00 within tolerance 7.394008e-07\n",
      "{0: ['rsm', 'sr.', 'national', 'east', 'business', 'midwest', 'interiors', 'region', 'central', 'west'], 1: ['aircraft', 'update', 'bulletin', 'information', 'technical', 'trim', '...', 'instrument', 'maintenance', 'cb'], 2: ['communications', 'corporate', 'fedak', 'heidi', 'promoted', '14all', 'baugniet', 'highlights', 'reporting', 'robert'], 3: ['updates', 'loading', 'charlie', 'certification', 'dls', 'modules', 'orientation', 'https', 'data', 'check']}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "t = 4\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,min_df=2,stop_words='english')\n",
    "X = vectorizer.fit_transform(summaryArr)\n",
    "km = KMeans(n_clusters = t, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)\n",
    "km.fit(X)\n",
    "np.unique(km.labels_, return_counts=True)\n",
    "text={}\n",
    "for i,cluster in enumerate(km.labels_):\n",
    "    oneDocument = summaryArr[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = oneDocument\n",
    "    else:\n",
    "        text[cluster] += oneDocument\n",
    "stopWords = set(stopwords.words('english')+list(punctuation))\n",
    "keywords = {}\n",
    "counts={}\n",
    "for cluster in range(t):\n",
    "    word_sent = word_tokenize(text[cluster].lower())\n",
    "    word_sent=[word for word in word_sent if word not in stopWords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(100, freq, key=freq.get)\n",
    "    counts[cluster]=freq\n",
    "uniqueKeys={}\n",
    "for cluster in range(t):   \n",
    "    other_clusters=list(set(range(t))-set([cluster]))\n",
    "    keys_other_clusters=set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "    unique=set(keywords[cluster])-keys_other_clusters\n",
    "    uniqueKeys[cluster]=nlargest(10, unique, key=counts[cluster].get)\n",
    "print(uniqueKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = \"\"\"A focus on safety\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post_vec = vectorizer.transform([post])\n",
    "new_post_label = km.predict(new_post_vec)[0]#We will predict it's cluster \n",
    "# Comparing all \n",
    "similar_indices = (km.labels_ == new_post_label).nonzero()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = []\n",
    "for i in similar_indices:\n",
    "    dist = sp.linalg.norm((new_post_vec - X[i]).toarray())\n",
    "    similar.append((dist, summaryArr[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "similar = sorted(similar) \n",
    "print(len(similar))\n",
    "show_at_1 = similar[0]\n",
    "show_at_2 = similar[2]\n",
    "show_at_3 = similar[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== #1 ===\n",
      "(1.2763020409819394, '1   Volume 16, Issue 19 November 14, 2008 Volume 16, Issue 19      \\n \\n \\n \\n   \\n \\n \\n \\n \\n   \\n \\n \\n \\n  A FOCUS ON SAFETY ALL:\\n The Gulfstream manuals now call only for bonding \\nthe fuel source to the aircraft during fueling or defueling...2                                   TOP STORIES ALL: \\nAn overview of the large-cabin AMM revisions \\nTechnical Publications has produced to date in 2008–\\n3  \\nAstraŽ/SPXŽ/G100\\n®/G150®/G200®: Rudder Trim \\ninstallations on the Astra through G200 aircraft are \\nsusceptible to water ingress into the trim actuator \\nhousings...4  G150®: ﬁGEN LOAD UNBALANCEﬂ status messages \\nhave been traced to a poor gr\\nound for the APU generator \\nreturn cable at 10GND...5  G200®: An issue surrounding the operational \\ncharacteristics of the Entr\\ny Light Switch Panel (CMS1 \\nCabin System) has been brought \\nto our attention by a \\nG200 operator...5  GII®/GIII®: During left engine start, when the start button \\nwas depressed, both engines began rotating...\\n6  GIV®/G300®/G400®: A GIV crew reported a control wheel \\noscillation related to the horiz\\nontal stabilizer.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== #1 ===\")\n",
    "print(show_at_1)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== #2 ===\n",
      "(1.4142135623730951, 'BREAKFAST MINUTES                              November 14, 2008    13 Ł \\nBreakfast Minutes Tips \\nŠ Here are some useful tips to help you get the most out of \\nBreakfast Minutes resources:  Œ Printing the entire \\nBreakfast Minutes issue Š\\n From myGulfstream.com, navigate to the \\nBreakfast Minutes home page (click Resources Œ> Breakfas\\nt Minutes), select the PDF Version for \\nthe particular issue you want to print (Adobe\\n® Acrobat\\n® Reader is required).')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== #2 ===\")\n",
    "print(show_at_2)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== #3 ===\n",
      "(1.4142135623730951, 'MAINTENANCE SALES TEAM  16  Regional Maintenance Sales Team \\nUpcoming Events November 17 Œ Forum, Reception, Dubai, UAE \\nDecember 3 Œ Forum, CMP.net, Glendale, Calif. \\nJune 8-11, 2009ŒGulfstream O\\nperators™ Conference\\nBREAKFAST MINUTES                              November 14, 2008    3TOP STORIES  Large-Cabin (ATA 00): Aircraft Maintenance Manual Update By Skip Weinrick, Sr.')\n"
     ]
    }
   ],
   "source": [
    "print(\"=== #3 ===\")\n",
    "print(show_at_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
